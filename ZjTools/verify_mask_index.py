#!/usr/bin/env python3
"""
éªŒè¯maskçš„token indexæ˜¯å¦ä¸SFTè®­ç»ƒæ—¶çš„æ•°æ®å¯¹åº”
æ¨¡æ‹ŸSFTè®­ç»ƒæ—¶çš„æ•°æ®å¤„ç†æµç¨‹ï¼Œæ£€æŸ¥maskç´¢å¼•æ˜ å°„æ˜¯å¦æ­£ç¡®
"""
import os
import sys
import torch
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from transformers import AutoTokenizer, TrainingArguments

# å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¯¼å…¥æ—¶çš„ä¾èµ–é—®é¢˜
def _import_llamafactory_modules():
    """å»¶è¿Ÿå¯¼å…¥llamafactoryæ¨¡å—ï¼Œé¿å…å¯¼å…¥æ—¶çš„ä¾èµ–é—®é¢˜"""
    try:
        from llamafactory.data.loader import _get_merged_dataset, _get_preprocessed_dataset
        from llamafactory.hparams import DataArguments, ModelArguments
        from llamafactory.data.template import get_template_and_fix_tokenizer
        from llamafactory.extras.constants import IGNORE_INDEX
        return _get_merged_dataset, _get_preprocessed_dataset, DataArguments, ModelArguments, get_template_and_fix_tokenizer, IGNORE_INDEX
    except ImportError as e:
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œå°è¯•ä¿®å¤mm_pluginçš„å¯¼å…¥é—®é¢˜
        if 'mllama' in str(e):
            # ä¸´æ—¶åˆ›å»ºä¸€ä¸ªå‡çš„mllamaæ¨¡å—
            import transformers.models
            if not hasattr(transformers.models, 'mllama'):
                class FakeMllama:
                    class processing_mllama:
                        pass
                transformers.models.mllama = FakeMllama()
            # é‡æ–°å¯¼å…¥
            from llamafactory.data.loader import _get_merged_dataset, _get_preprocessed_dataset
            from llamafactory.hparams import DataArguments, ModelArguments
            from llamafactory.data.template import get_template_and_fix_tokenizer
            from llamafactory.extras.constants import IGNORE_INDEX
            return _get_merged_dataset, _get_preprocessed_dataset, DataArguments, ModelArguments, get_template_and_fix_tokenizer, IGNORE_INDEX
        raise


# åœ¨å‡½æ•°ä¸­ä½¿ç”¨æ—¶å¯¼å…¥
_get_merged_dataset = None
_get_preprocessed_dataset = None
DataArguments = None
ModelArguments = None
get_template_and_fix_tokenizer = None
IGNORE_INDEX = -100  # é»˜è®¤å€¼

def load_mask_file(mask_dir, sample_idx):
    """åŠ è½½maskæ–‡ä»¶"""
    mask_file = Path(mask_dir) / f"sample_{sample_idx}_mask.pt"
    if not mask_file.exists():
        return None
    return torch.load(mask_file, map_location="cpu")


def simulate_sft_preprocessing(data_args, template, tokenizer, sample_idx, model_path, apply_mask=False, mask_dir=None):
    """æ¨¡æ‹ŸSFTè®­ç»ƒæ—¶çš„æ•°æ®é¢„å¤„ç†æµç¨‹
    
    Args:
        apply_mask: æ˜¯å¦åº”ç”¨maskï¼ˆç”¨äºéªŒè¯maskåº”ç”¨åçš„ç»“æœï¼‰
        mask_dir: maskæ–‡ä»¶ç›®å½•ï¼ˆå¦‚æœapply_mask=Trueï¼‰
        model_path: æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºModelArgumentsï¼‰
    """
    # åˆ›å»ºç®€å•çš„training_argsç”¨äºæ•°æ®åŠ è½½
    training_args = TrainingArguments(
        output_dir="./tmp",
        per_device_train_batch_size=1,
        remove_unused_columns=False,
    )
    # è®¾ç½®predict_with_generateå±æ€§ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not hasattr(training_args, 'predict_with_generate'):
        training_args.predict_with_generate = False
    
    model_args = ModelArguments(model_name_or_path=model_path)
    
    # è·å–åˆå¹¶åçš„æ•°æ®é›†
    dataset = _get_merged_dataset(
        data_args.dataset,
        model_args,
        data_args,
        training_args,
        stage="sft",
    )
    
    if dataset is None or len(dataset) == 0:
        return None
    
    if sample_idx >= len(dataset):
        print(f"âš ï¸  æ ·æœ¬ç´¢å¼• {sample_idx} è¶…å‡ºæ•°æ®é›†å¤§å° {len(dataset)}")
        return None
    
    # ä¸´æ—¶ç§»é™¤attr_mask_dirï¼Œå› ä¸ºæˆ‘ä»¬æƒ³æ‰‹åŠ¨æ§åˆ¶maskåº”ç”¨
    original_attr_mask_dir = data_args.attr_mask_dir
    if not apply_mask:
        data_args.attr_mask_dir = None
    
    # é¢„å¤„ç†æ•°æ®é›†ï¼ˆè¿™ä¼šåº”ç”¨tokenizationå’Œtemplateï¼‰
    dataset = _get_preprocessed_dataset(
        dataset,
        data_args,
        training_args,
        stage="sft",
        template=template,
        tokenizer=tokenizer,
        processor=None,
        is_eval=False,
    )
    
    # æ¢å¤attr_mask_dir
    data_args.attr_mask_dir = original_attr_mask_dir
    
    if dataset is None or len(dataset) == 0:
        return None
    
    if sample_idx >= len(dataset):
        print(f"âš ï¸  æ ·æœ¬ç´¢å¼• {sample_idx} è¶…å‡ºé¢„å¤„ç†åæ•°æ®é›†å¤§å° {len(dataset)}")
        return None
    
    # å¦‚æœéœ€è¦åœ¨é¢„å¤„ç†æ—¶åº”ç”¨maskï¼ˆæ¨¡æ‹Ÿå®é™…è®­ç»ƒæµç¨‹ï¼‰
    if apply_mask and mask_dir:
        from pathlib import Path
        mask_file = Path(mask_dir) / f"sample_{sample_idx}_mask.pt"
        if mask_file.exists():
            payload = torch.load(mask_file, map_location="cpu")
            example = dataset[sample_idx]
            labels = example["labels"]
            if isinstance(labels, list):
                labels = torch.tensor(labels)
            
            first_valid = next((i for i, v in enumerate(labels) if v != IGNORE_INDEX), None)
            resp_start = payload.get("response_start_idx", first_valid or 0)
            if first_valid is not None and first_valid != resp_start:
                resp_start = first_valid
            
            mask_tensor = payload.get("mask", None)
            if isinstance(mask_tensor, torch.Tensor):
                mask_tensor = mask_tensor.tolist()
            ignore_value = payload.get("ignore_value", -100)
            
            for rel, val in enumerate(mask_tensor):
                if val == ignore_value:
                    target = resp_start + rel
                    if 0 <= target < len(labels):
                        labels[target] = IGNORE_INDEX
                    else: 
                        raise ValueError(f"Maskç´¢å¼•è¶Šç•Œ: rel={rel}, target={target}, labelsé•¿åº¦={len(labels)}")
            example["labels"] = labels.tolist() if isinstance(labels, torch.Tensor) else labels
            return example
    
    # è·å–æŒ‡å®šæ ·æœ¬ï¼ˆä¸åº”ç”¨maskï¼‰
    example = dataset[sample_idx]
    
    return example


def verify_mask_application(example, mask_data, sample_idx, verbose=True):
    """éªŒè¯maskåº”ç”¨æ˜¯å¦æ­£ç¡®"""
    if example is None or mask_data is None:
        return False
    
    labels = example["labels"]
    if isinstance(labels, list):
        labels = torch.tensor(labels)
    
    # è·å–å®é™…çš„responseèµ·å§‹ä½ç½®ï¼ˆç¬¬ä¸€ä¸ªéIGNORE_INDEXçš„ä½ç½®ï¼‰
    first_valid = next((i for i, v in enumerate(labels) if v != IGNORE_INDEX), None)
    
    # ä»maskæ–‡ä»¶è·å–çš„ä¿¡æ¯
    mask_resp_start = mask_data.get("response_start_idx", None)
    mask_tensor = mask_data.get("mask", None)
    ignore_value = mask_data.get("ignore_value", -100)
    mask_response_length = mask_data.get("response_length", None)
    
    if isinstance(mask_tensor, torch.Tensor):
        mask_tensor = mask_tensor.tolist()
    
    # è®¡ç®—å®é™…çš„responseé•¿åº¦
    if first_valid is not None:
        actual_resp_start = first_valid
        actual_resp_length = (labels != IGNORE_INDEX).sum().item() #é€šè¿‡åŸå…ˆlabelsä¸­ä¸æ˜¯maskçš„æ•°é‡æ¥è®¡ç®—çš„
    else:
        actual_resp_start = 0
        actual_resp_length = 0
    
    # éªŒè¯ä¿¡æ¯
    results = {
        "sample_idx": sample_idx,
        "mask_resp_start": mask_resp_start,
        "actual_resp_start": actual_resp_start,
        "mask_resp_length": mask_response_length,
        "actual_resp_length": actual_resp_length,
        "mask_tensor_length": len(mask_tensor),
        "labels_length": len(labels),
        "match": True,
        "issues": []
    }
    
    # æ£€æŸ¥1: responseèµ·å§‹ä½ç½®æ˜¯å¦åŒ¹é…
    if mask_resp_start is not None:
        if first_valid is not None and first_valid != mask_resp_start:
            results["match"] = False
            results["issues"].append(
                f"Responseèµ·å§‹ä½ç½®ä¸åŒ¹é…: maskä¸­={mask_resp_start}, å®é™…={first_valid}"
            )
            # ä½¿ç”¨å®é™…çš„èµ·å§‹ä½ç½®ï¼ˆä»£ç ä¸­çš„é€»è¾‘ï¼‰
            resp_start = first_valid
        else:
            resp_start = mask_resp_start
    else:
        resp_start = first_valid or 0
    
    # æ£€æŸ¥2: mask tensoré•¿åº¦æ˜¯å¦ä¸responseé•¿åº¦åŒ¹é…
    if mask_response_length is not None:
        if mask_response_length != len(mask_tensor):
            results["match"] = False
            results["issues"].append(
                f"Mask tensoré•¿åº¦ä¸response_lengthä¸åŒ¹é…: "
                f"mask_response_length={mask_response_length}, mask_tensoré•¿åº¦={len(mask_tensor)}"
            )
    
    if actual_resp_length > 0 and len(mask_tensor) != actual_resp_length:
        results["match"] = False
        results["issues"].append(
            f"Mask tensoré•¿åº¦ä¸å®é™…responseé•¿åº¦ä¸åŒ¹é…: "
            f"mask_tensoré•¿åº¦={len(mask_tensor)}, å®é™…responseé•¿åº¦={actual_resp_length}"
        )
    
    # æ£€æŸ¥3: æ¨¡æ‹Ÿmaskåº”ç”¨è¿‡ç¨‹
    masked_positions = []
    for rel, val in enumerate(mask_tensor):
        if val == ignore_value:
            target = resp_start + rel
            if 0 <= target < len(labels):
                masked_positions.append(target)
            else:
                results["match"] = False
                results["issues"].append(
                    f"Maskç´¢å¼•è¶Šç•Œ: rel={rel}, target={target}, labelsé•¿åº¦={len(labels)}"
                )
    
    results["masked_count"] = len(masked_positions)
    results["masked_positions"] = masked_positions[:10]  # åªä¿å­˜å‰10ä¸ªç”¨äºæ˜¾ç¤º
    
    # æ£€æŸ¥4: éªŒè¯maskåçš„labels
    if verbose:
        print(f"\n{'='*60}")
        print(f"æ ·æœ¬ {sample_idx} çš„MaskéªŒè¯ç»“æœ")
        print(f"{'='*60}")
        print(f"ğŸ“‹ åŸºæœ¬ä¿¡æ¯:")
        print(f"   - Labelsæ€»é•¿åº¦: {len(labels)}")
        print(f"   - Maskä¸­çš„response_start_idx: {mask_resp_start}")
        print(f"   - å®é™…çš„responseèµ·å§‹ä½ç½®: {actual_resp_start}")
        print(f"   - ä½¿ç”¨çš„responseèµ·å§‹ä½ç½®: {resp_start}")
        print(f"   - Maskä¸­çš„response_length: {mask_response_length}")
        print(f"   - Mask tensoré•¿åº¦: {len(mask_tensor)}")
        print(f"   - å®é™…responseé•¿åº¦: {actual_resp_length}")
        
        print(f"\nğŸ¯ Maskåº”ç”¨æ£€æŸ¥:")
        print(f"   - è¢«maskçš„tokenæ•°é‡: {len(masked_positions)}")
        if len(masked_positions) > 0:
            print(f"   - å‰10ä¸ªè¢«maskçš„ä½ç½®: {masked_positions[:10]}")
        
        if results["issues"]:
            print(f"\nå‘ç°çš„é—®é¢˜:")
            for issue in results["issues"]:
                print(f"   - {issue}")
        else:
            print(f"\næ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼")
    
    return results


def main():
    parser = argparse.ArgumentParser(description="éªŒè¯maskçš„token indexæ˜¯å¦ä¸SFTè®­ç»ƒæ•°æ®å¯¹åº”")
    parser.add_argument("--sample_idx", type=int, default=0, help="è¦éªŒè¯çš„æ ·æœ¬ç´¢å¼•")
    parser.add_argument("--mask_dir", type=str,
                       default="saves/qwen3-4b/full/attr_temp1.0/masks",
                       help="Maskæ–‡ä»¶ç›®å½•")
    parser.add_argument("--model_path", type=str,
                       default="/mnt/zj-gpfs/home/whs/model/Qwen3-4B-Instruct-2507",
                       help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--dataset", type=str, default="math_cot",
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--template", type=str, default="qwen",
                       help="æ¨¡æ¿åç§°")
    parser.add_argument("--cutoff_len", type=int, default=18000,
                       help="æˆªæ–­é•¿åº¦")
    parser.add_argument("--packing", action="store_true",
                       help="æ˜¯å¦ä½¿ç”¨packing")
    # seedå‚æ•°åœ¨TrainingArgumentsä¸­ï¼Œè¿™é‡Œä¸éœ€è¦
    parser.add_argument("--batch_samples", type=int, nargs="+", default=None,
                       help="æ‰¹é‡éªŒè¯å¤šä¸ªæ ·æœ¬ï¼Œä¾‹å¦‚: --batch_samples 0 10 50 100")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("Maskç´¢å¼•éªŒè¯å·¥å…·")
    print("=" * 60)
    
    # å¯¼å…¥llamafactoryæ¨¡å—
    global _get_merged_dataset, _get_preprocessed_dataset, DataArguments, ModelArguments, get_template_and_fix_tokenizer, IGNORE_INDEX
    try:
        _get_merged_dataset, _get_preprocessed_dataset, DataArguments, ModelArguments, get_template_and_fix_tokenizer, IGNORE_INDEX = _import_llamafactory_modules()
    except Exception as e:
        print(f"âŒ å¯¼å…¥llamafactoryæ¨¡å—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # å‡†å¤‡æ•°æ®å‚æ•°ï¼ˆéœ€è¦å…ˆåˆ›å»ºdata_argsï¼Œå› ä¸ºtemplateéœ€è¦å®ƒï¼‰
    print(f"\n[1/3] å‡†å¤‡æ•°æ®å‚æ•°")
    data_args = DataArguments(
        dataset=args.dataset,
        template=args.template,
        cutoff_len=args.cutoff_len,
        packing=args.packing,
        overwrite_cache=True,
        preprocessing_num_workers=1,  # éªŒè¯æ—¶ä½¿ç”¨å•è¿›ç¨‹
    )
    
    # åŠ è½½tokenizerå’Œtemplate
    print(f"\n[2/3] åŠ è½½æ¨¡å‹å’Œtokenizer: {args.model_path}")
    try:
        # ç›´æ¥ä½¿ç”¨AutoTokenizeråŠ è½½ï¼Œé¿å…å¤æ‚çš„å¯¼å…¥
        tokenizer = AutoTokenizer.from_pretrained(
            args.model_path,
            trust_remote_code=True,
            padding_side="right"
        )
        template = get_template_and_fix_tokenizer(
            tokenizer=tokenizer,
            data_args=data_args
        )
        print("âœ… Tokenizerå’ŒtemplateåŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    print(f"   - æ•°æ®é›†: {args.dataset}")
    print(f"   - æ¨¡æ¿: {args.template}")
    print(f"   - æˆªæ–­é•¿åº¦: {args.cutoff_len}")
    print(f"   - Packing: {args.packing}")
    
    # ç¡®å®šè¦éªŒè¯çš„æ ·æœ¬åˆ—è¡¨
    if args.batch_samples:
        sample_indices = args.batch_samples
    else:
        sample_indices = [args.sample_idx]
    
    print(f"\n[3/4] éªŒè¯æ ·æœ¬: {sample_indices}")
    
    all_results = []
    for sample_idx in sample_indices:
        print(f"\n{'='*60}")
        print(f"éªŒè¯æ ·æœ¬ {sample_idx}")
        print(f"{'='*60}")
        
        # åŠ è½½maskæ–‡ä»¶
        mask_data = load_mask_file(args.mask_dir, sample_idx)
        if mask_data is None:
            print(f"âŒ æ— æ³•åŠ è½½maskæ–‡ä»¶: {args.mask_dir}/sample_{sample_idx}_mask.pt")
            continue
        
        # æ¨¡æ‹ŸSFTé¢„å¤„ç†ï¼ˆä¸åº”ç”¨maskï¼Œç”¨äºéªŒè¯maskç´¢å¼•ï¼‰
        try:
            example = simulate_sft_preprocessing(
                data_args, template, tokenizer, sample_idx, args.model_path,
                apply_mask=False, mask_dir=None
            )
            if example is None:
                print(f"âŒ æ— æ³•è·å–æ ·æœ¬ {sample_idx}")
                continue
        except Exception as e:
            print(f"âŒ é¢„å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
        
        # ä¹Ÿè·å–åº”ç”¨maskåçš„æ ·æœ¬ç”¨äºå¯¹æ¯”
        try:
            example_with_mask = simulate_sft_preprocessing(
                data_args, template, tokenizer, sample_idx, args.model_path,
                apply_mask=True, mask_dir=args.mask_dir
            )
        except Exception as e:
            example_with_mask = None
            print(f"âš ï¸  æ— æ³•è·å–åº”ç”¨maskåçš„æ ·æœ¬: {e}")
        
        # éªŒè¯maskåº”ç”¨
        result = verify_mask_application(example, mask_data, sample_idx, verbose=True)
        
        # å¦‚æœæˆåŠŸè·å–äº†åº”ç”¨maskåçš„æ ·æœ¬ï¼Œè¿›è¡Œé¢å¤–éªŒè¯
        if example_with_mask is not None:
            labels_before = example["labels"]
            labels_after = example_with_mask["labels"]
            if isinstance(labels_before, list):
                labels_before = torch.tensor(labels_before)
            if isinstance(labels_after, list):
                labels_after = torch.tensor(labels_after)
            
            # æ£€æŸ¥maskæ˜¯å¦æ­£ç¡®åº”ç”¨
            before_ignore_count = (labels_before == IGNORE_INDEX).sum().item()
            after_ignore_count = (labels_after == IGNORE_INDEX).sum().item()
            additional_masked = after_ignore_count - before_ignore_count
            
            print(f"å¯¹æ¯”åº”ç”¨maskå‰å:")
            print(f"   - åº”ç”¨maskå‰IGNORE_INDEXæ•°é‡: {before_ignore_count}")
            print(f"   - åº”ç”¨maskåIGNORE_INDEXæ•°é‡: {after_ignore_count}")
            print(f"   - æ–°å¢maskçš„tokenæ•°é‡: {additional_masked}")
            print(f"   - Maskæ–‡ä»¶ä¸­çš„maskedæ•°é‡: {result.get('masked_count', 0)}")
            
            if additional_masked != result.get('masked_count', 0):
                result["match"] = False
                result["issues"].append(
                    f"Maskåº”ç”¨æ•°é‡ä¸åŒ¹é…: å®é™…æ–°å¢={additional_masked}, "
                    f"é¢„æœŸ={result.get('masked_count', 0)}"
                )
        
        all_results.append(result)
    
    # æ€»ç»“
    print(f"\n{'='*60}")
    print("éªŒè¯æ€»ç»“")
    print(f"{'='*60}")
    
    success_count = sum(1 for r in all_results if r and r.get("match", False))
    total_count = len(all_results)
    
    print(f"æ€»æ ·æœ¬æ•°: {total_count}")
    print(f"éªŒè¯é€šè¿‡: {success_count}")
    print(f"éªŒè¯å¤±è´¥: {total_count - success_count}")
    
    if success_count < total_count:
        print(f"\nâš ï¸  éƒ¨åˆ†æ ·æœ¬éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é—®é¢˜")
    else:
        print(f"\nâœ… æ‰€æœ‰æ ·æœ¬éªŒè¯é€šè¿‡ï¼")


if __name__ == "__main__":
    main()

