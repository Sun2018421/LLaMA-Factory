## model
model_name_or_path: /mnt/zj-gpfs/home/whs/model/Qwen3-4B-Instruct-2507
enable_liger_kernel: true
### method
stage: sft
do_train: true
finetuning_type: full
# use_r1_step_mask: true
### ddp
ddp_timeout: 180000000
deepspeed: /examples/deepspeed/ds_z3_config.json
flash_attn: fa2
### dataset
dataset: math_cot
template: qwen
cutoff_len: 18000
packing: false
attr_mask_dir: /mnt/zj-gpfs/home/sunxiaofeng/Reason_related/LLaMA-Factory/saves/qwen3-4b/full/attr_temp1.0/masks

# packing: true
#drop_exceed_length_data: true
#sequence_parallel_size: 1
# rope_scaling: linear
# max_samples: 100
overwrite_cache: true
preprocessing_num_workers: 32
seed: 42
### output
output_dir: /mnt/zj-gpfs/home/sunxiaofeng/Reason_related/LLaMA-Factory/saves/qwen3-4b/full/results/
logging_steps: 1
# save_steps: 100
save_strategy: epoch
# save_strategy: steps
# save_steps: 50
save_total_limit: 2
overwrite_output_dir: false
save_only_model: true
plot_loss: true
# seed: 42
# resume_from_checkpoint: true
#overwrite_output_dir: true
### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
learning_rate: 5.0e-5
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-8
max_grad_norm: 1.0
weight_decay: 0.1
num_train_epochs: 6.0
lr_scheduler_type: cosine_with_min_lr
lr_scheduler_kwargs: {'min_lr': 1.0e-5}
warmup_ratio: 0.1
bf16: true
